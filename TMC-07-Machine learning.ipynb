{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Supervised vs unsupervised learning\n",
    "### Cross validation and evaluation metrics\n",
    "\n",
    "1. Holdout test set - how model will perform on unseen real world data\n",
    "2. K- fold cross validation: Data is divided into k subsets and , one of k subsets is used as holdout set and k-1 are clubbed together for training\n",
    "3. Accuracy = # predicted correctly/ total # of ovservations\n",
    "4. Precision = # predicted as spam that are actually spam(true positives)/# perdicted as spam(ture positives+ false positives)\n",
    "5. Recall = # predicted as spam that are actually spam(true positives)/# actually spam(true positives + false negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest model\n",
    "\n",
    "Ensemble learning model, creates multiple decision trees and combines their outcomes to produce beter results than any single model\n",
    "\n",
    "#### Benefits\n",
    "1. Can be used for classification or regression\n",
    "2. Easily handles outliers and missing values, etc\n",
    "3. Accepts various types of inputs(continuous, ordinal etc)\n",
    "4. Less Likely to overfit\n",
    "5. Outputs feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>body_per_punct</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8327</th>\n",
       "      <th>8328</th>\n",
       "      <th>8329</th>\n",
       "      <th>8330</th>\n",
       "      <th>8331</th>\n",
       "      <th>8332</th>\n",
       "      <th>8333</th>\n",
       "      <th>8334</th>\n",
       "      <th>8335</th>\n",
       "      <th>8336</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  body_per_punct    0    1    2    3    4    5    6    7  ...  \\\n",
       "0       160             2.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1       128             4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2        49             4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3        62             3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4        28             7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "   8327  8328  8329  8330  8331  8332  8333  8334  8335  8336  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8339 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read clean and vectorize the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Reading the raw file\n",
    "\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t', header = None)\n",
    "data.columns = ['label','body_text']\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Cleaning the text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = ([word for word in tokens if word not in nltk.corpus.stopwords.words(\"English\")])\n",
    "    #text = [wn.lemmatize(word) for word in text]   \n",
    "    text = [ps.stem(word) for word in text] \n",
    "    return(text)\n",
    "\n",
    "\n",
    "# Counting the percennt of punctuations\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round((count/(len(text)-text.count(\" \"))),3)*100\n",
    "\n",
    "# Counting the length\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "# Adding punctuation percent column\n",
    "data['body_per_punct'] = data['body_text'].apply(lambda x: count_punct(x)) \n",
    "\n",
    "data.head()\n",
    "\n",
    "# Creating the TfIdf term document matrix\n",
    "Tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "X_count = Tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "X_features = pd.concat([data['body_len'],data['body_per_punct'], pd.DataFrame(X_count.toarray())], axis = 1)\n",
    "\n",
    "\n",
    "X_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Random forest classifier Attributes and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_estimator_type', '_get_param_names', '_get_tags', '_make_estimator', '_more_tags', '_required_parameters', '_set_oob_score', '_validate_X_predict', '_validate_estimator', '_validate_y_class_weight', 'apply', 'decision_path', 'feature_importances_', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params']\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(dir(RandomForestClassifier))\n",
    "print(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features\n",
    "feature_importance, fit , predict\n",
    "\n",
    "### Hyperparameters\n",
    "max_depth, n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore random forest classifier through cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97486535, 0.96768402, 0.97486535, 0.95867026, 0.97124888])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf =  RandomForestClassifier(n_jobs = -1) # parallel processing: building the individual jobs in parallel\n",
    "k_fold = KFold(n_splits=5)\n",
    "cross_val_score(rf, X_features, data['label'], cv = k_fold, scoring = 'accuracy', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore random forest classifier through Holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_features, data['label'], test_size = 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "rf_model = rf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.06327530214330236, 2017),\n",
       " (0.03878606408453409, 7577),\n",
       " (0.02959121840078095, 3349),\n",
       " (0.029156006732044264, 'body_len'),\n",
       " (0.022292717985557924, 6503),\n",
       " (0.021189055965959425, 6965),\n",
       " (0.018915676865765438, 392),\n",
       " (0.018653226124106387, 7689),\n",
       " (0.018434064115584816, 5294),\n",
       " (0.016880909378177246, 2384),\n",
       " (0.01669367914778382, 7247),\n",
       " (0.0165520706356305, 2244),\n",
       " (0.01525221238828935, 1112),\n",
       " (0.015084139607667104, 8013),\n",
       " (0.01422020842910521, 5009),\n",
       " (0.014063696776728689, 5945),\n",
       " (0.013278346634162462, 2511),\n",
       " (0.012551869404204606, 6207),\n",
       " (0.010740175502087912, 'body_per_punct'),\n",
       " (0.00995598298213791, 695),\n",
       " (0.009445533143461249, 3658),\n",
       " (0.009292329684735872, 4511),\n",
       " (0.008854100588432588, 1572),\n",
       " (0.008499358520544087, 294),\n",
       " (0.008324557126902112, 397),\n",
       " (0.007521493126467247, 6269),\n",
       " (0.007306754466829286, 7445),\n",
       " (0.00662561432947275, 2294),\n",
       " (0.0065115264153367625, 4485),\n",
       " (0.006319488517861582, 878),\n",
       " (0.006318302811504844, 7733),\n",
       " (0.006117122795669002, 4594),\n",
       " (0.005819835069163959, 7771),\n",
       " (0.005810483032735002, 1531),\n",
       " (0.005693666293931829, 3307),\n",
       " (0.0056454731190256945, 5221),\n",
       " (0.005547487985265128, 1081),\n",
       " (0.0054392463225353335, 295),\n",
       " (0.00518701462152749, 2096),\n",
       " (0.0051382109816653, 360),\n",
       " (0.00511917705757881, 8277),\n",
       " (0.0049343055651779625, 8294),\n",
       " (0.004708227584334175, 1022),\n",
       " (0.004350882950132606, 1106),\n",
       " (0.0042240471587176275, 8065),\n",
       " (0.004170744645835465, 7191),\n",
       " (0.004154300524812938, 2046),\n",
       " (0.004143360805966453, 2308),\n",
       " (0.0039837607893325865, 3007),\n",
       " (0.003897445357947607, 7545),\n",
       " (0.0037998245620755517, 471),\n",
       " (0.003779356399772007, 2155),\n",
       " (0.00361341559354999, 4947),\n",
       " (0.0035172202204191315, 7922),\n",
       " (0.003515485693915521, 5337),\n",
       " (0.0034593263801057615, 368),\n",
       " (0.0034316436673031705, 6509),\n",
       " (0.0034312919223679706, 1570),\n",
       " (0.003409350755160408, 6054),\n",
       " (0.0033470307093266287, 2417),\n",
       " (0.00326052308264419, 2727),\n",
       " (0.0032416208158963073, 6347),\n",
       " (0.0032322308983684443, 7671),\n",
       " (0.003206884131927599, 5817),\n",
       " (0.003204886650838035, 1438),\n",
       " (0.0031736732924265054, 2637),\n",
       " (0.0031386879256632993, 818),\n",
       " (0.0031292397568968818, 5414),\n",
       " (0.0030816089042630334, 5864),\n",
       " (0.0029062105569321157, 442),\n",
       " (0.0029019593525626075, 793),\n",
       " (0.0028758108481270746, 437),\n",
       " (0.00286791429205708, 2563),\n",
       " (0.00282140132909004, 5506),\n",
       " (0.0028074460496327284, 615),\n",
       " (0.0027805180002728525, 7818),\n",
       " (0.0027744395757385142, 53),\n",
       " (0.0027740232633980716, 5674),\n",
       " (0.0027429602971071037, 5791),\n",
       " (0.0027096475121299803, 5006),\n",
       " (0.002683612199322399, 4025),\n",
       " (0.0026589073499868037, 6469),\n",
       " (0.0025926616457404177, 440),\n",
       " (0.002579887104154666, 1114),\n",
       " (0.0024070986785683967, 3105),\n",
       " (0.002349621941146232, 2139),\n",
       " (0.0022758895465258807, 292),\n",
       " (0.002265345855257686, 2461),\n",
       " (0.0022208826409336455, 6136),\n",
       " (0.002206973793028011, 982),\n",
       " (0.002200742497033668, 1118),\n",
       " (0.0022004764825748237, 5772),\n",
       " (0.0021637586977776326, 696),\n",
       " (0.0021570114194055527, 2044),\n",
       " (0.0021513852392404725, 2712),\n",
       " (0.0021190514227573294, 3057),\n",
       " (0.002000996092156597, 2146),\n",
       " (0.001982111526955565, 5215),\n",
       " (0.001975071161308843, 5717),\n",
       " (0.0019283352495583544, 1191),\n",
       " (0.0019014869263665874, 4484),\n",
       " (0.0018987173338251644, 3440),\n",
       " (0.001855634977134964, 1116),\n",
       " (0.001787769580476995, 7919),\n",
       " (0.0017288916236142635, 5488),\n",
       " (0.0017143052397345163, 6094),\n",
       " (0.001704840324743536, 426),\n",
       " (0.001674254608958945, 7686),\n",
       " (0.0016704327487330301, 1007),\n",
       " (0.0016678483095528668, 7605),\n",
       " (0.0016467530029174845, 5359),\n",
       " (0.0016414373008415426, 1863),\n",
       " (0.0016319947715224604, 5051),\n",
       " (0.0016021550820546516, 375),\n",
       " (0.0015558026367745738, 7070),\n",
       " (0.0015149143403970639, 1224),\n",
       " (0.0015127204605590832, 311),\n",
       " (0.0015027104548173938, 821),\n",
       " (0.0014497417338296362, 2835),\n",
       " (0.0013742910556764187, 7656),\n",
       " (0.0013742519764584657, 7663),\n",
       " (0.0013193597673462768, 3924),\n",
       " (0.0013047159881092185, 2816),\n",
       " (0.0012997877896954717, 6480),\n",
       " (0.0012970530462164192, 5510),\n",
       " (0.0012957127254353772, 5942),\n",
       " (0.0012814538753339855, 7467),\n",
       " (0.0012649005686005302, 4708),\n",
       " (0.0012396614735618948, 1080),\n",
       " (0.0012382705284492417, 7590),\n",
       " (0.0011905092624259878, 1104),\n",
       " (0.0011863252032328225, 4162),\n",
       " (0.0011813077638658314, 50),\n",
       " (0.0011641521734203852, 6212),\n",
       " (0.0011605603395169614, 5812),\n",
       " (0.0011277164517836964, 450),\n",
       " (0.0011183038702195646, 3883),\n",
       " (0.0011125648823170405, 4600),\n",
       " (0.0010997097747679199, 475),\n",
       " (0.0010562102021829986, 6453),\n",
       " (0.0010491642490957351, 354),\n",
       " (0.0010417952871570853, 2993),\n",
       " (0.0010406555001295588, 8156),\n",
       " (0.0010384077057862904, 6117),\n",
       " (0.0010336255537200995, 4281),\n",
       " (0.001011647878721787, 2937),\n",
       " (0.0010063659638104758, 5469),\n",
       " (0.0009994678335243023, 338),\n",
       " (0.000994523238732656, 1998),\n",
       " (0.0009944770973684177, 4428),\n",
       " (0.0009923608612012005, 6765),\n",
       " (0.0009916805451409927, 3001),\n",
       " (0.000981808628299059, 6958),\n",
       " (0.0009770398211886501, 729),\n",
       " (0.0009699045642085158, 976),\n",
       " (0.0009697047445926011, 7774),\n",
       " (0.0009677672029767182, 3154),\n",
       " (0.0009543454320632866, 1034),\n",
       " (0.0009539764108597226, 7596),\n",
       " (0.0009486361062626128, 6595),\n",
       " (0.0009446394205230769, 4098),\n",
       " (0.0009213036297382479, 296),\n",
       " (0.000920545747540348, 4059),\n",
       " (0.0009155276501473862, 3294),\n",
       " (0.0009110095740938041, 1833),\n",
       " (0.0009006167542418148, 7373),\n",
       " (0.0008969688793267028, 4839),\n",
       " (0.0008478989251299779, 2802),\n",
       " (0.0008429527594572815, 649),\n",
       " (0.0008426621945692071, 364),\n",
       " (0.0008324771552885912, 3493),\n",
       " (0.000829180213856089, 5169),\n",
       " (0.0008132707793834286, 645),\n",
       " (0.000808277614423521, 2097),\n",
       " (0.0007975260932220979, 4853),\n",
       " (0.0007922870470944737, 54),\n",
       " (0.0007859800588615689, 5809),\n",
       " (0.0007765247391608912, 7869),\n",
       " (0.0007697384512566064, 325),\n",
       " (0.0007639527323245132, 7522),\n",
       " (0.0007629063490208341, 6930),\n",
       " (0.0007511302977878377, 8199),\n",
       " (0.000748489259508779, 981),\n",
       " (0.0007459914856860785, 443),\n",
       " (0.0007396647928193195, 5727),\n",
       " (0.0007388469633006489, 8100),\n",
       " (0.000737636933170699, 1265),\n",
       " (0.0007356880673926011, 191),\n",
       " (0.0007355962679873101, 550),\n",
       " (0.0007322683021839621, 549),\n",
       " (0.000726144775771992, 5753),\n",
       " (0.0007204728343788204, 3534),\n",
       " (0.0007196328664204296, 7316),\n",
       " (0.000709476389618086, 1394),\n",
       " (0.0007024767077819301, 5065),\n",
       " (0.0007021262927067915, 4370),\n",
       " (0.0006935582437159013, 2505),\n",
       " (0.0006737951986590444, 3564),\n",
       " (0.0006703180369456529, 8048),\n",
       " (0.0006628347651707709, 3395),\n",
       " (0.0006624491714923843, 787),\n",
       " (0.0006557109331595506, 1019),\n",
       " (0.0006547111792271045, 1540),\n",
       " (0.0006538932308713827, 7057),\n",
       " (0.0006521888126086018, 5765),\n",
       " (0.0006496540453520834, 1176),\n",
       " (0.0006495966469238218, 5339),\n",
       " (0.0006477428913378455, 975),\n",
       " (0.000636959078605381, 2537),\n",
       " (0.0006276935554731278, 7707),\n",
       " (0.000626782484834072, 344),\n",
       " (0.0006266966241674559, 93),\n",
       " (0.0006150334862775052, 1501),\n",
       " (0.0006109467187707949, 7234),\n",
       " (0.0006011235458351057, 5421),\n",
       " (0.0005987882891476908, 4585),\n",
       " (0.0005958452907168004, 744),\n",
       " (0.0005870427144822973, 7217),\n",
       " (0.0005855810800251624, 6201),\n",
       " (0.0005842194045237967, 324),\n",
       " (0.0005816039243162943, 3045),\n",
       " (0.0005775686966141024, 5366),\n",
       " (0.0005717871500925134, 5314),\n",
       " (0.0005690348634970462, 2764),\n",
       " (0.000558657594780521, 1102),\n",
       " (0.0005549314060855171, 4916),\n",
       " (0.000551581258824465, 293),\n",
       " (0.0005495730681236259, 4239),\n",
       " (0.000549045897654174, 2222),\n",
       " (0.0005465851868722415, 5826),\n",
       " (0.0005426914636177181, 1987),\n",
       " (0.0005425576753649365, 4042),\n",
       " (0.000539243090050158, 5246),\n",
       " (0.0005373398832239064, 5391),\n",
       " (0.0005361193715242129, 2347),\n",
       " (0.0005357188147762307, 118),\n",
       " (0.0005215969920383107, 820),\n",
       " (0.0005164833718216949, 211),\n",
       " (0.0005148959709303176, 4850),\n",
       " (0.0005095194545895825, 4054),\n",
       " (0.0005092768257294066, 1378),\n",
       " (0.0005075734668619704, 1484),\n",
       " (0.0005043993874069754, 1092),\n",
       " (0.0004997607120074461, 2033),\n",
       " (0.0004935571323867182, 3824),\n",
       " (0.0004908089254054898, 6404),\n",
       " (0.0004906455081294891, 3715),\n",
       " (0.0004904760298940474, 152),\n",
       " (0.0004895754597109811, 4616),\n",
       " (0.0004889072014792474, 2608),\n",
       " (0.0004888310750882013, 5042),\n",
       " (0.00048578605071359, 8196),\n",
       " (0.000481545520684717, 5081),\n",
       " (0.0004762094380485719, 1132),\n",
       " (0.0004750656659791335, 5501),\n",
       " (0.00047451859665854635, 2509),\n",
       " (0.00047056434678164634, 1033),\n",
       " (0.0004694930544078506, 90),\n",
       " (0.0004690095471901964, 907),\n",
       " (0.0004666584996907083, 8249),\n",
       " (0.00046294763554144023, 5477),\n",
       " (0.000462232385343707, 1103),\n",
       " (0.0004592974490402512, 4068),\n",
       " (0.000459185256279843, 7735),\n",
       " (0.0004578048309862054, 4700),\n",
       " (0.000457016930856972, 6831),\n",
       " (0.000456487129155018, 3690),\n",
       " (0.00045187955297150846, 6643),\n",
       " (0.00045069039336579264, 167),\n",
       " (0.0004469241344041625, 556),\n",
       " (0.0004460070133145875, 3826),\n",
       " (0.00044442809635290446, 8246),\n",
       " (0.00044386482483358927, 5626),\n",
       " (0.0004423824252154473, 938),\n",
       " (0.00043988420962211907, 6914),\n",
       " (0.00043821338311014605, 833),\n",
       " (0.000435323950069228, 6830),\n",
       " (0.00043480396306980423, 7321),\n",
       " (0.0004341119364942668, 7280),\n",
       " (0.0004335262136680815, 883),\n",
       " (0.00043116143914566753, 2273),\n",
       " (0.0004297976885672697, 6251),\n",
       " (0.0004244551434688677, 3236),\n",
       " (0.00042291427415506734, 3229),\n",
       " (0.0004226712192619687, 2141),\n",
       " (0.00042072502740110265, 6247),\n",
       " (0.00041334224743283835, 8044),\n",
       " (0.0004132723062818392, 551),\n",
       " (0.00040943917483416846, 888),\n",
       " (0.00040486756774693003, 4894),\n",
       " (0.00040450703041215556, 8037),\n",
       " (0.0004044784849388999, 840),\n",
       " (0.00040379044856763557, 4158),\n",
       " (0.0004037353356962586, 459),\n",
       " (0.00040288495935022724, 5463),\n",
       " (0.0004018430796611664, 3587),\n",
       " (0.00040149799127324525, 4751),\n",
       " (0.0004014605733240755, 5754),\n",
       " (0.00039057662633582284, 2386),\n",
       " (0.00038814797353508857, 7440),\n",
       " (0.00038607233242100863, 909),\n",
       " (0.0003855955226909801, 2475),\n",
       " (0.00038511130744928374, 7230),\n",
       " (0.0003844670311006905, 78),\n",
       " (0.00037952498825197625, 577),\n",
       " (0.00037924105510060593, 4973),\n",
       " (0.0003789638935058527, 6307),\n",
       " (0.0003783761197984465, 2312),\n",
       " (0.000375098342771101, 5039),\n",
       " (0.0003745663813798818, 2630),\n",
       " (0.0003745303962189796, 850),\n",
       " (0.00036844323611135365, 5012),\n",
       " (0.0003677790849824518, 6799),\n",
       " (0.00036758427695966897, 6459),\n",
       " (0.00036677657592830816, 3925),\n",
       " (0.00036592593830025923, 2332),\n",
       " (0.0003658911871530073, 3225),\n",
       " (0.00036520815207161746, 7537),\n",
       " (0.00036256705937638787, 7024),\n",
       " (0.00035664565328556497, 7319),\n",
       " (0.0003551114153530736, 2741),\n",
       " (0.0003549846325684133, 6267),\n",
       " (0.0003515422793209082, 1211),\n",
       " (0.00035141355642656364, 1057),\n",
       " (0.0003512401824073884, 4893),\n",
       " (0.0003502231179210856, 2051),\n",
       " (0.0003499506507835348, 5298),\n",
       " (0.0003474966267495966, 1600),\n",
       " (0.0003399590823495093, 586),\n",
       " (0.00033751811419926894, 1591),\n",
       " (0.00033673984367240507, 5121),\n",
       " (0.0003364377814090267, 623),\n",
       " (0.00033345930084085996, 534),\n",
       " (0.0003332458347722854, 3796),\n",
       " (0.00033250180067450626, 5140),\n",
       " (0.0003323581059834667, 6407),\n",
       " (0.00033184484219205153, 3361),\n",
       " (0.0003309349652293424, 1671),\n",
       " (0.0003280978775414703, 1878),\n",
       " (0.0003257637239982592, 1312),\n",
       " (0.00032432245843734477, 7088),\n",
       " (0.00032345458784546326, 4712),\n",
       " (0.00032295908315193087, 7093),\n",
       " (0.00031923666823812204, 7306),\n",
       " (0.0003181473917615892, 6925),\n",
       " (0.0003176346392848145, 6197),\n",
       " (0.0003147083811490309, 8164),\n",
       " (0.0003139803492560982, 5766),\n",
       " (0.00031279928623302225, 6704),\n",
       " (0.00031200480349974634, 7263),\n",
       " (0.00030812565565258, 4799),\n",
       " (0.0003079754125892949, 4152),\n",
       " (0.00030789254785191497, 5802),\n",
       " (0.0003075054456165322, 7395),\n",
       " (0.0003074822779004172, 5052),\n",
       " (0.0003069523398991941, 4685),\n",
       " (0.00030621388323227064, 868),\n",
       " (0.00030287479376133993, 901),\n",
       " (0.00030286918117863507, 7740),\n",
       " (0.0003022293966717007, 588),\n",
       " (0.0003006731409993039, 3795),\n",
       " (0.00030054776342922806, 6756),\n",
       " (0.00029985346043586987, 4045),\n",
       " (0.00029886416876775295, 7411),\n",
       " (0.00029794707561068315, 7155),\n",
       " (0.0002971840468121713, 6059),\n",
       " (0.0002936226089737735, 7893),\n",
       " (0.00029327480861996867, 7193),\n",
       " (0.0002922187260679394, 1951),\n",
       " (0.00029182077007000427, 583),\n",
       " (0.00029176208522070364, 61),\n",
       " (0.000290144903871823, 5194),\n",
       " (0.00028942010584431213, 153),\n",
       " (0.0002885439767433183, 4786),\n",
       " (0.00028840232377071694, 1353),\n",
       " (0.00028717766604792054, 4102),\n",
       " (0.00028676588169291597, 2028),\n",
       " (0.0002857175042726628, 8151),\n",
       " (0.00028428545772008184, 881),\n",
       " (0.00028403727577019423, 4637),\n",
       " (0.00028203220317883296, 0),\n",
       " (0.00028129355077184513, 238),\n",
       " (0.00028033640216629653, 869),\n",
       " (0.00027871347592120353, 4654),\n",
       " (0.00027805158520676903, 2995),\n",
       " (0.00027598845829034677, 2),\n",
       " (0.0002754814780226328, 5808),\n",
       " (0.00027249162405934264, 3689),\n",
       " (0.00027205082126949195, 7557),\n",
       " (0.00027069945746833454, 226),\n",
       " (0.0002698939955561024, 2265),\n",
       " (0.0002697174253660541, 8142),\n",
       " (0.00026729830094434845, 6828),\n",
       " (0.0002660312023946569, 5780),\n",
       " (0.00026461403069778734, 7272),\n",
       " (0.00026420981312425456, 370),\n",
       " (0.0002624388621706336, 217),\n",
       " (0.00026241342796115336, 992),\n",
       " (0.00026240922551132675, 6281),\n",
       " (0.00026188280096830114, 558),\n",
       " (0.00026061418797121097, 5825),\n",
       " (0.00025830721979728887, 3087),\n",
       " (0.0002561944478996634, 5518),\n",
       " (0.0002544949806234792, 4694),\n",
       " (0.0002540590494442432, 2074),\n",
       " (0.0002537560557765011, 3487),\n",
       " (0.00025222785531946504, 574),\n",
       " (0.0002504315934102977, 740),\n",
       " (0.00024910232531471534, 4722),\n",
       " (0.000248144480649165, 5535),\n",
       " (0.0002478542564374973, 680),\n",
       " (0.00024762373530613676, 444),\n",
       " (0.00024739726914515815, 892),\n",
       " (0.00024655816740680627, 2521),\n",
       " (0.0002463571068371579, 3357),\n",
       " (0.0002457402606786877, 4650),\n",
       " (0.0002453648775987306, 8054),\n",
       " (0.00024478890282537155, 874),\n",
       " (0.00024238790565388738, 597),\n",
       " (0.00024173328582005577, 855),\n",
       " (0.0002404361012325984, 6110),\n",
       " (0.00024007217991112162, 310),\n",
       " (0.00023947781855841808, 194),\n",
       " (0.00023922552887244768, 169),\n",
       " (0.00023351302975422886, 1762),\n",
       " (0.00023319449997342146, 4398),\n",
       " (0.00023207747604532764, 5384),\n",
       " (0.00023037699120650727, 5231),\n",
       " (0.00022914837297252557, 7582),\n",
       " (0.00022806435105334223, 4558),\n",
       " (0.00022695548649832818, 8234),\n",
       " (0.00022689059590991025, 5798),\n",
       " (0.0002253391698460974, 1130),\n",
       " (0.00022460303843497686, 900),\n",
       " (0.0002240489603199026, 3350),\n",
       " (0.00022387990686812583, 6880),\n",
       " (0.00022274664818930248, 5226),\n",
       " (0.00022167886157486197, 4367),\n",
       " (0.00022158398064396176, 1125),\n",
       " (0.00022108200392833676, 6644),\n",
       " (0.0002203289572895231, 2573),\n",
       " (0.0002199022878659015, 6767),\n",
       " (0.00021924453572529028, 435),\n",
       " (0.00021799355478519645, 1349),\n",
       " (0.00021777443499085947, 1061),\n",
       " (0.0002170421358192333, 1042),\n",
       " (0.00021641688471020628, 3957),\n",
       " (0.00021635731895875186, 7342),\n",
       " (0.00021554193682764595, 4733),\n",
       " (0.00021504523352381164, 2205),\n",
       " (0.00021465632859299822, 8323),\n",
       " (0.00021399690297314666, 2317),\n",
       " (0.00021374829501196246, 2699),\n",
       " (0.00021329727028543822, 4184),\n",
       " (0.00021255814720828235, 4189),\n",
       " (0.00021000950481443587, 8183),\n",
       " (0.00020956607592563522, 1947),\n",
       " (0.00020932972646144673, 4608),\n",
       " (0.000208818367509531, 6587),\n",
       " (0.0002066522423881522, 6814),\n",
       " (0.00020572608736362338, 847),\n",
       " (0.00020509942631465034, 5978),\n",
       " (0.00020453047605562585, 4838),\n",
       " (0.00020428391608766773, 5435),\n",
       " (0.00020350303968986826, 3377),\n",
       " (0.00020241554031847047, 7834),\n",
       " (0.00020169827418403373, 8176),\n",
       " (0.00019823742468321395, 3790),\n",
       " (0.00019808236528209418, 3246),\n",
       " (0.00019667363861340314, 229),\n",
       " (0.00019658515108800657, 7897),\n",
       " (0.000196392867017782, 2670),\n",
       " (0.0001954060165696772, 8221),\n",
       " (0.0001943977777717912, 1734),\n",
       " (0.00019422613930406154, 936),\n",
       " (0.000194013971695237, 13),\n",
       " (0.00019336186208775572, 1672),\n",
       " (0.0001922113277790428, 6783),\n",
       " (0.00019142356981510504, 2260),\n",
       " (0.00019004334524018972, 99),\n",
       " (0.0001900108048640954, 5032),\n",
       " (0.00018969818926495958, 3770),\n",
       " (0.00018906412659383247, 3035),\n",
       " (0.0001888504153947826, 609),\n",
       " (0.00018880694924957537, 6138),\n",
       " (0.00018870995752606417, 4806),\n",
       " (0.00018817738918471772, 287),\n",
       " (0.00018813156322204324, 3412),\n",
       " (0.00018719095901623955, 7934),\n",
       " (0.00018712588420474814, 4148),\n",
       " (0.000187077889309577, 63),\n",
       " (0.00018523806229936356, 2352),\n",
       " (0.00018474693127959026, 1004),\n",
       " (0.0001844254597409765, 3870),\n",
       " (0.0001843880784916266, 1072),\n",
       " (0.00018411851512576002, 4129),\n",
       " (0.00018411764413903042, 871),\n",
       " (0.00018375537604625684, 6271),\n",
       " (0.00018256397224368308, 1207),\n",
       " (0.00018213065690826646, 1264),\n",
       " (0.00018211667146176782, 2555),\n",
       " (0.000181922971512325, 448),\n",
       " (0.0001819226311130681, 6520),\n",
       " (0.00018171287007282897, 7962),\n",
       " (0.00018160388995051166, 1978),\n",
       " (0.00018103139684164094, 5489),\n",
       " (0.00017968474008935316, 2906),\n",
       " (0.00017948198197455606, 2433),\n",
       " (0.00017886121929079624, 7846),\n",
       " (0.00017863663057817415, 3354),\n",
       " (0.00017825218391744467, 7016),\n",
       " (0.00017676384586010153, 1099),\n",
       " (0.0001755841272147087, 8185),\n",
       " (0.0001749429682830746, 4534),\n",
       " (0.00017428175437999175, 5748),\n",
       " (0.0001741012164690875, 4318),\n",
       " (0.00017362274899169225, 6523),\n",
       " (0.00017322981828962156, 7437),\n",
       " (0.0001716360710152994, 117),\n",
       " (0.00017162506833331387, 4422),\n",
       " (0.00017161982217323853, 1066),\n",
       " (0.00017108188944195844, 204),\n",
       " (0.00017099426582413798, 2157),\n",
       " (0.00017097330810367099, 1290),\n",
       " (0.00017088692965109106, 4635),\n",
       " (0.000170316720078454, 6489),\n",
       " (0.0001693925606076074, 3950),\n",
       " (0.00016873941878317277, 6495),\n",
       " (0.00016799274514144928, 4939),\n",
       " (0.00016760304516124847, 827),\n",
       " (0.0001674770935698367, 2313),\n",
       " (0.00016721422783008328, 5029),\n",
       " (0.00016639156579386583, 1235),\n",
       " (0.00016603639461896036, 7432),\n",
       " (0.00016587150368501584, 8020),\n",
       " (0.00016549360020658244, 1278),\n",
       " (0.00016465659027933442, 3730),\n",
       " (0.00016436043206103778, 5168),\n",
       " (0.00016434914098169666, 381),\n",
       " (0.0001640493416924985, 3934),\n",
       " (0.0001629147357102131, 4208),\n",
       " (0.00016287363413345816, 121),\n",
       " (0.00016253514313412895, 8112),\n",
       " (0.0001608522122446651, 2156),\n",
       " (0.0001591993370424417, 6615),\n",
       " (0.0001591158475138454, 4809),\n",
       " (0.0001579806427263959, 6204),\n",
       " (0.0001571489632031683, 6721),\n",
       " (0.00015676849984925628, 1845),\n",
       " (0.00015614761033693568, 4720),\n",
       " (0.0001560231564687802, 420),\n",
       " (0.0001556597649779098, 8029),\n",
       " (0.00015500587068265244, 249),\n",
       " (0.0001548683941263247, 2651),\n",
       " (0.00015176914642844405, 4310),\n",
       " (0.00015137564427799592, 6173),\n",
       " (0.0001510346051024496, 3726),\n",
       " (0.0001505204964271278, 142),\n",
       " (0.00015033149981007955, 6510),\n",
       " (0.00015013184477170484, 267),\n",
       " (0.00015000505929681297, 1417),\n",
       " (0.0001491934836913534, 8083),\n",
       " (0.0001479407366730621, 6576),\n",
       " (0.00014768869293467908, 2062),\n",
       " (0.00014755993612417328, 6953),\n",
       " (0.00014746511948947956, 7116),\n",
       " (0.0001471260205715444, 6784),\n",
       " (0.0001466702796716176, 4390),\n",
       " (0.0001465585297431256, 1076),\n",
       " (0.00014582881585633843, 2799),\n",
       " (0.00014400314329408075, 3887),\n",
       " (0.00014022352277728276, 5925),\n",
       " (0.0001392263249739097, 7483),\n",
       " (0.0001390175242694077, 6021),\n",
       " (0.00013859024280441268, 1901),\n",
       " (0.0001383415238888139, 990),\n",
       " (0.00013724493894596779, 4713),\n",
       " (0.00013713733480836982, 782),\n",
       " (0.0001369338525673725, 4067),\n",
       " (0.0001367682487466883, 262),\n",
       " (0.00013670706889588587, 141),\n",
       " (0.000136182601216732, 7693),\n",
       " (0.00013489041528642523, 3704),\n",
       " (0.00013465857943677497, 8262),\n",
       " (0.00013395795544728108, 7797),\n",
       " (0.00013330924095242326, 3062),\n",
       " (0.00013275336049412627, 830),\n",
       " (0.0001324078001712908, 7512),\n",
       " (0.0001311823190274815, 8200),\n",
       " (0.0001301744517281938, 2609),\n",
       " (0.0001299026500106084, 1185),\n",
       " (0.0001294939434523456, 4293),\n",
       " (0.00012862641576851953, 1727),\n",
       " (0.0001283582483690009, 2430),\n",
       " (0.0001281826536310215, 2696),\n",
       " (0.00012817942181950876, 3000),\n",
       " (0.00012796080375272975, 1058),\n",
       " (0.00012773569469652445, 1049),\n",
       " (0.00012730121145079485, 3121),\n",
       " (0.0001268428113939606, 1341),\n",
       " (0.0001267520875830278, 2108),\n",
       " (0.00012672858218465871, 1660),\n",
       " (0.0001263716171269789, 7370),\n",
       " (0.00012632370780397438, 7398),\n",
       " (0.0001260526247964339, 248),\n",
       " (0.00012578243806399137, 8102),\n",
       " (0.00012457376416807734, 4509),\n",
       " (0.00012436673349249764, 9),\n",
       " (0.00012435318351725907, 4391),\n",
       " (0.00012408719401510164, 1767),\n",
       " (0.00012405012001210765, 1005),\n",
       " (0.00012391904906148377, 4855),\n",
       " (0.0001231758285137189, 3515),\n",
       " (0.00012313881610627757, 2760),\n",
       " (0.00012300032769878367, 6375),\n",
       " (0.00012291781148398472, 7699),\n",
       " (0.0001228789496275949, 5452),\n",
       " (0.00012287367545375265, 3615),\n",
       " (0.0001226162988672363, 314),\n",
       " (0.00012221382869802638, 3562),\n",
       " (0.00012148557151314786, 5082),\n",
       " (0.00012133753528352502, 4796),\n",
       " (0.00012130139554411134, 7882),\n",
       " (0.0001212473867602929, 5935),\n",
       " (0.00012083706287862952, 956),\n",
       " (0.00012071333001897038, 860),\n",
       " (0.00012039512448269809, 4179),\n",
       " (0.0001200898288496038, 8154),\n",
       " (0.00011962704886490325, 6306),\n",
       " (0.0001187872390744358, 2131),\n",
       " (0.00011870804974050658, 6973),\n",
       " (0.00011870305359898763, 5721),\n",
       " (0.00011817316567945179, 6240),\n",
       " (0.00011812364775587473, 115),\n",
       " (0.00011764213267602979, 222),\n",
       " (0.00011759638857492647, 5095),\n",
       " (0.00011739004064327253, 8308),\n",
       " (0.0001173226411918123, 7061),\n",
       " (0.00011722187822996506, 4666),\n",
       " (0.00011717681219174972, 1613),\n",
       " (0.00011710227093100992, 8106),\n",
       " (0.000116628692964503, 2170),\n",
       " (0.00011657648229953006, 1924),\n",
       " (0.00011549199551813544, 6918),\n",
       " (0.00011513102705537822, 8189),\n",
       " (0.00011479508442166579, 7179),\n",
       " (0.00011418202573226094, 8329),\n",
       " (0.00011415035111009216, 647),\n",
       " (0.00011374463694495601, 7202),\n",
       " (0.00011319956736881073, 4889),\n",
       " (0.000113158765971436, 560),\n",
       " (0.0001130784336069426, 4183),\n",
       " (0.0001126625929924613, 373),\n",
       " (0.00011256456209828799, 5008),\n",
       " (0.00011227471799055887, 5673),\n",
       " (0.00011214574368178742, 3753),\n",
       " (0.00011213525500381035, 1339),\n",
       " (0.00011210567334724503, 853),\n",
       " (0.0001116926945387709, 2344),\n",
       " (0.00011165510064808608, 7327),\n",
       " (0.00011158957095199745, 7523),\n",
       " (0.00011092113493074474, 25),\n",
       " (0.00011072267804811631, 1117),\n",
       " (0.00011061830536807424, 1813),\n",
       " (0.00010954153608682227, 6447),\n",
       " (0.00010913522293623203, 7579),\n",
       " (0.0001089724614337632, 7825),\n",
       " (0.00010885829273481841, 6297),\n",
       " (0.00010880377025961615, 6993),\n",
       " (0.00010856482039178635, 2951),\n",
       " (0.00010802052331402354, 1120),\n",
       " (0.00010661135897471184, 334),\n",
       " (0.00010635029820831982, 3367),\n",
       " (0.00010530765590499067, 1812),\n",
       " (0.0001051206532927317, 6650),\n",
       " (0.0001049664564962109, 774),\n",
       " (0.00010440522108476242, 8001),\n",
       " (0.00010388317281088285, 3190),\n",
       " (0.00010381425174970678, 1088),\n",
       " (0.00010375862315084897, 1077),\n",
       " (0.00010220494076739729, 5390),\n",
       " (0.00010218769801346868, 923),\n",
       " (0.00010203485040913652, 7122),\n",
       " (0.00010168770637993697, 784),\n",
       " (0.00010135860892356312, 4363),\n",
       " (0.000101035227832637, 283),\n",
       " (0.00010078430292885172, 5130),\n",
       " (0.0001007800095025865, 439),\n",
       " (0.00010074592152188942, 3126),\n",
       " (0.00010058101663756043, 4198),\n",
       " (0.00010042551422951848, 5153),\n",
       " (0.00010036091394412118, 816),\n",
       " (9.999155455737463e-05, 7284),\n",
       " (9.992053655496408e-05, 6865),\n",
       " (9.968285806294546e-05, 490),\n",
       " (9.9616475703955e-05, 5022),\n",
       " (9.9615570620447e-05, 524),\n",
       " (9.951478859603123e-05, 5200),\n",
       " (9.885158471350548e-05, 1029),\n",
       " (9.882937945161924e-05, 8098),\n",
       " (9.870685583149474e-05, 4863),\n",
       " (9.846470626991226e-05, 7062),\n",
       " (9.844229827251578e-05, 349),\n",
       " (9.842334796491762e-05, 2263),\n",
       " (9.83146417217218e-05, 5731),\n",
       " (9.803171254931034e-05, 4513),\n",
       " (9.799060414415873e-05, 341),\n",
       " (9.73322540099939e-05, 4099),\n",
       " (9.674141958371532e-05, 5728),\n",
       " (9.592560626875421e-05, 3518),\n",
       " (9.588000858265288e-05, 6948),\n",
       " (9.561652748288265e-05, 6505),\n",
       " (9.55891959404116e-05, 710),\n",
       " (9.511741760069e-05, 636),\n",
       " (9.479959706642947e-05, 6810),\n",
       " (9.474168045317711e-05, 2635),\n",
       " (9.461508852440695e-05, 8211),\n",
       " (9.451363992001648e-05, 7705),\n",
       " (9.415159545862037e-05, 1109),\n",
       " (9.350666172163931e-05, 6030),\n",
       " (9.310400620591777e-05, 6706),\n",
       " (9.305651093227985e-05, 1317),\n",
       " (9.285299028014322e-05, 7141),\n",
       " (9.253431248057244e-05, 4862),\n",
       " (9.229107657582754e-05, 3155),\n",
       " (9.212181035002411e-05, 8069),\n",
       " (9.184014235505404e-05, 7540),\n",
       " (9.182901510689587e-05, 5512),\n",
       " (9.160182097566085e-05, 2718),\n",
       " (9.136319830587805e-05, 2118),\n",
       " (9.101872107513183e-05, 2018),\n",
       " (9.09923325702985e-05, 6975),\n",
       " (9.026263521092098e-05, 3053),\n",
       " (8.999822195003958e-05, 2375),\n",
       " (8.986612893405615e-05, 6599),\n",
       " (8.979080215498139e-05, 7969),\n",
       " (8.966818615106701e-05, 468),\n",
       " (8.930494036290946e-05, 1582),\n",
       " (8.909341012892012e-05, 3329),\n",
       " (8.882017699011814e-05, 2291),\n",
       " (8.84146668668769e-05, 3026),\n",
       " (8.768713196865778e-05, 1030),\n",
       " (8.667509975798935e-05, 7980),\n",
       " (8.62633240891255e-05, 502),\n",
       " (8.619095999177633e-05, 2564),\n",
       " (8.482368261658248e-05, 7933),\n",
       " (8.459305529971982e-05, 2525),\n",
       " (8.44046096298301e-05, 4878),\n",
       " (8.374357651648433e-05, 8071),\n",
       " (8.338125992596635e-05, 931),\n",
       " (8.337082107015408e-05, 44),\n",
       " (8.178088441725847e-05, 1460),\n",
       " (8.174563987783568e-05, 942),\n",
       " (8.148701098099075e-05, 5348),\n",
       " (8.090937644432522e-05, 289),\n",
       " (8.015743312197107e-05, 144),\n",
       " (8.005124908628511e-05, 2847),\n",
       " (7.99379861145633e-05, 610),\n",
       " (7.960837470836037e-05, 2988),\n",
       " (7.958760124226864e-05, 8288),\n",
       " (7.935937500453996e-05, 2400),\n",
       " (7.907358185711896e-05, 8218),\n",
       " (7.88145686487685e-05, 4866),\n",
       " (7.852723207061339e-05, 3287),\n",
       " (7.829228506834828e-05, 2148),\n",
       " (7.809002753749918e-05, 5416),\n",
       " (7.798494301532894e-05, 5541),\n",
       " (7.796088354024108e-05, 3056),\n",
       " (7.779522672295397e-05, 6081),\n",
       " (7.705577123374431e-05, 3314),\n",
       " (7.644083206639348e-05, 1343),\n",
       " (7.599615059469529e-05, 7013),\n",
       " (7.579722379353864e-05, 1583),\n",
       " (7.563013206673205e-05, 7989),\n",
       " (7.563013206673205e-05, 714),\n",
       " (7.528421030460891e-05, 1060),\n",
       " (7.512064399024988e-05, 6431),\n",
       " (7.492914981115227e-05, 6770),\n",
       " (7.492463185004953e-05, 4962),\n",
       " (7.486722524122603e-05, 2778),\n",
       " (7.469591434915238e-05, 6077),\n",
       " (7.464926894084173e-05, 8235),\n",
       " (7.412551992933759e-05, 197),\n",
       " (7.409981074497316e-05, 8086),\n",
       " (7.382999117732973e-05, 6047),\n",
       " (7.372401201307151e-05, 1361),\n",
       " (7.337478840239844e-05, 6161),\n",
       " (7.330220652832202e-05, 7384),\n",
       " (7.260604305959954e-05, 260),\n",
       " (7.25036316913941e-05, 4670),\n",
       " (7.21389713871435e-05, 8332),\n",
       " (7.142923672540607e-05, 3419),\n",
       " (7.122854844907183e-05, 5618),\n",
       " (7.120376210616232e-05, 616),\n",
       " (7.083111423599871e-05, 7416),\n",
       " (7.077855682500544e-05, 4111),\n",
       " (7.049738049941325e-05, 937),\n",
       " (7.017264196395577e-05, 573),\n",
       " (7.010281606889027e-05, 8204),\n",
       " (6.995926269805946e-05, 6353),\n",
       " (6.963199915602179e-05, 1356),\n",
       " (6.945032829009934e-05, 617),\n",
       " (6.944330664375449e-05, 4545),\n",
       " (6.932653239671327e-05, 7587),\n",
       " (6.912997360884842e-05, 2180),\n",
       " (6.884817535477913e-05, 7716),\n",
       " (6.866536737070074e-05, 5410),\n",
       " (6.799983380548052e-05, 8261),\n",
       " (6.773800603810325e-05, 3601),\n",
       " (6.772626660478611e-05, 98),\n",
       " (6.755667652417975e-05, 240),\n",
       " (6.7434626645327e-05, 4134),\n",
       " (6.732042997806791e-05, 6202),\n",
       " (6.708443612115142e-05, 210),\n",
       " (6.699486121318745e-05, 7431),\n",
       " (6.694240954020535e-05, 1752),\n",
       " (6.687850826538217e-05, 3683),\n",
       " (6.66439937643424e-05, 4090),\n",
       " (6.642906360572885e-05, 7811),\n",
       " (6.600091021527492e-05, 3532),\n",
       " (6.577498075326424e-05, 5573),\n",
       " (6.570204103320307e-05, 6050),\n",
       " (6.563216890387362e-05, 3499),\n",
       " (6.561175314090461e-05, 4226),\n",
       " (6.550275359382007e-05, 1638),\n",
       " (6.532500424651205e-05, 6149),\n",
       " (6.529884443697205e-05, 3652),\n",
       " (6.507450507638154e-05, 6464),\n",
       " (6.496746647018482e-05, 5444),\n",
       " (6.464321999383227e-05, 5621),\n",
       " (6.45309611903566e-05, 1021),\n",
       " (6.449394047581031e-05, 8078),\n",
       " (6.431879286837239e-05, 1040),\n",
       " (6.424581681189963e-05, 7089),\n",
       " (6.387411074090251e-05, 5683),\n",
       " (6.385378565793366e-05, 5988),\n",
       " (6.364792546966397e-05, 6649),\n",
       " (6.355850751949171e-05, 2309),\n",
       " (6.354508830882782e-05, 1254),\n",
       " (6.307543769720029e-05, 7480),\n",
       " (6.301946201909231e-05, 8149),\n",
       " (6.29190338687306e-05, 3989),\n",
       " (6.285665893330923e-05, 8089),\n",
       " (6.283799213766859e-05, 6840),\n",
       " (6.275698588264671e-05, 7486),\n",
       " (6.225120320899512e-05, 350),\n",
       " (6.217776390172093e-05, 4073),\n",
       " (6.197571912036337e-05, 557),\n",
       " (6.180774989147746e-05, 6232),\n",
       " (6.175617577870216e-05, 961),\n",
       " (6.163400321787318e-05, 411),\n",
       " (6.15009905835366e-05, 3086),\n",
       " (6.146452004778968e-05, 3827),\n",
       " (6.134624189471462e-05, 2530),\n",
       " (6.134150849661407e-05, 2624),\n",
       " (6.11779138363505e-05, 2003),\n",
       " (6.11514668545011e-05, 1957),\n",
       " (6.107173682062899e-05, 3298),\n",
       " (6.0914907180745156e-05, 2368),\n",
       " (6.079876000359987e-05, 1645),\n",
       " (6.077477182210021e-05, 1902),\n",
       " (6.0774213267909664e-05, 6804),\n",
       " (6.0698652079043655e-05, 8152),\n",
       " (6.065751166416059e-05, 2056),\n",
       " (6.051002795436864e-05, 8009),\n",
       " (6.048008843565723e-05, 4314),\n",
       " (6.0451857915826014e-05, 1877),\n",
       " (6.038736622339139e-05, 4537),\n",
       " (6.0248168586184804e-05, 756),\n",
       " (5.9644902437074545e-05, 799),\n",
       " (5.957547875393786e-05, 7228),\n",
       " (5.9371790850776954e-05, 7490),\n",
       " (5.923410628551287e-05, 5550),\n",
       " (5.9105753226733675e-05, 4676),\n",
       " (5.8698673661920124e-05, 8068),\n",
       " (5.8603789029463315e-05, 653),\n",
       " (5.817531030257363e-05, 6854),\n",
       " (5.815701497408217e-05, 6199),\n",
       " (5.807172462331353e-05, 4770),\n",
       " (5.800380416273747e-05, 7133),\n",
       " (5.7823070084604385e-05, 734),\n",
       " (5.780121771672857e-05, 760),\n",
       " (5.7773172937628746e-05, 5660),\n",
       " (5.76641420874824e-05, 331),\n",
       " (5.7583816045072425e-05, 4371),\n",
       " (5.756090417781566e-05, 2817),\n",
       " (5.7520984068282636e-05, 5530),\n",
       " (5.7488657286943795e-05, 5367),\n",
       " (5.739377700796579e-05, 6506),\n",
       " (5.720459846156333e-05, 3242),\n",
       " (5.702160121110927e-05, 6152),\n",
       " (5.652327628653061e-05, 5579),\n",
       " (5.642835559405382e-05, 1205),\n",
       " (5.6263469346038596e-05, 1047),\n",
       " (5.591555345938136e-05, 5069),\n",
       " (5.571954649815345e-05, 5320),\n",
       " (5.571635302873279e-05, 3480),\n",
       " (5.5284552507016735e-05, 7410),\n",
       " (5.526746279581694e-05, 1698),\n",
       " (5.517761292595426e-05, 6984),\n",
       " (5.482840297538989e-05, 3556),\n",
       " (5.474404828704993e-05, 5026),\n",
       " (5.4669634457834334e-05, 7516),\n",
       " (5.419133669191659e-05, 4555),\n",
       " (5.414356053205677e-05, 7806),\n",
       " (5.409074954211138e-05, 7676),\n",
       " (5.374812980110505e-05, 2723),\n",
       " (5.359720056383826e-05, 5275),\n",
       " (5.3553927632164266e-05, 6405),\n",
       " (5.346748435773205e-05, 1604),\n",
       " (5.344952559356822e-05, 8180),\n",
       " (5.3425407413902756e-05, 3625),\n",
       " (5.330222123810462e-05, 7732),\n",
       " (5.329210619407089e-05, 4771),\n",
       " (5.294925330573847e-05, 2508),\n",
       " (5.2667517491157885e-05, 7862),\n",
       " (5.250967621274277e-05, 1984),\n",
       " (5.243528345668093e-05, 4748),\n",
       " (5.222868836470471e-05, 101),\n",
       " (5.192568922016203e-05, 1598),\n",
       " (5.179310201123107e-05, 4699),\n",
       " (5.158561289455065e-05, 108),\n",
       " (5.1549170406273636e-05, 239),\n",
       " (5.145328702468623e-05, 7867),\n",
       " (5.1414066025479754e-05, 3880),\n",
       " (5.13677596216123e-05, 2454),\n",
       " (5.099316536694909e-05, 8187),\n",
       " (5.087175689342414e-05, 5296),\n",
       " (5.086577583523154e-05, 1275),\n",
       " (5.079465944539567e-05, 4093),\n",
       " (5.064056331372763e-05, 3456),\n",
       " (5.063531037564776e-05, 4615),\n",
       " (5.052400383966434e-05, 6635),\n",
       " (5.038830216983252e-05, 7901),\n",
       " (5.038087809191542e-05, 2617),\n",
       " (5.034970377885493e-05, 7861),\n",
       " (5.03441771709261e-05, 29),\n",
       " (4.974127340690364e-05, 6541),\n",
       " (4.952308546168902e-05, 19),\n",
       " (4.948262096458676e-05, 3747),\n",
       " (4.930005896130133e-05, 7214),\n",
       " (4.905217481449585e-05, 8007),\n",
       " (4.8972136358963845e-05, 694),\n",
       " (4.8858154846375385e-05, 5870),\n",
       " (4.83207622618138e-05, 5785),\n",
       " (4.827537391464626e-05, 6328),\n",
       " (4.8233141900606944e-05, 2588),\n",
       " (4.819562750723213e-05, 851),\n",
       " (4.803637397729657e-05, 66),\n",
       " (4.787691637230218e-05, 5429),\n",
       " (4.780503160698398e-05, 6244),\n",
       " (4.740390284740511e-05, 2977),\n",
       " (4.72452682632291e-05, 2582),\n",
       " (4.6962543314959546e-05, 1798),\n",
       " (4.694965668763773e-05, 1958),\n",
       " (4.691774025914746e-05, 1535),\n",
       " (4.6872838766394726e-05, 7529),\n",
       " (4.678952074830896e-05, 6016),\n",
       " (4.673312029814592e-05, 4783),\n",
       " (4.657433991798649e-05, 604),\n",
       " (4.650898135997823e-05, 4026),\n",
       " (4.6448427916409826e-05, 7433),\n",
       " (4.6072620013079284e-05, 3232),\n",
       " (4.592007117752706e-05, 1059),\n",
       " (4.586858568923564e-05, 1023),\n",
       " (4.5235910603584246e-05, 3546),\n",
       " (4.5083061932603504e-05, 964),\n",
       " (4.4982065774020146e-05, 6259),\n",
       " (4.496996979704997e-05, 959),\n",
       " (4.4966767693294e-05, 8279),\n",
       " (4.452972743141508e-05, 4463),\n",
       " (4.451355122672522e-05, 7802),\n",
       " (4.44228613016292e-05, 1035),\n",
       " (4.44042212623489e-05, 2336),\n",
       " (4.439379587596232e-05, 794),\n",
       " (4.425406274526036e-05, 7045),\n",
       " (4.408204546241064e-05, 7911),\n",
       " (4.399991032959659e-05, 7576),\n",
       " (4.386507028888187e-05, 139),\n",
       " (4.346673655753001e-05, 2164),\n",
       " (4.344853622942829e-05, 5743),\n",
       " (4.336790319557316e-05, 8334),\n",
       " (4.295764200522803e-05, 829),\n",
       " (4.2901319525810875e-05, 2274),\n",
       " (4.26554186721241e-05, 6590),\n",
       " (4.2382457068441684e-05, 838),\n",
       " (4.2282346936417636e-05, 6423),\n",
       " (4.2219469315983316e-05, 5801),\n",
       " (4.216934805575785e-05, 337),\n",
       " (4.2138178794606265e-05, 3594),\n",
       " (4.210252520875041e-05, 7075),\n",
       " (4.201674003707336e-05, 6484),\n",
       " (4.2016740037073355e-05, 3102),\n",
       " (4.2016740037073355e-05, 2838),\n",
       " (4.191123326347567e-05, 6049),\n",
       " (4.189571194613606e-05, 8011),\n",
       " (4.1845153537164594e-05, 5045),\n",
       " (4.175102999061534e-05, 406),\n",
       " (4.171982914547973e-05, 2361),\n",
       " (4.171681958815771e-05, 755),\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:1.0 / Recall:0.573 / Accuracy: 0.9344703770197487 / fscore: 0.729\n"
     ]
    }
   ],
   "source": [
    "Y_pred = rf_model.predict(X_test)\n",
    "\n",
    "precision,recall,fscore,support = score(Y_test, Y_pred, pos_label = 'spam', average = 'binary')\n",
    "\n",
    "print('Precision:{} / Recall:{} / Accuracy: {} / fscore: {}'.format(round(precision,3), round(recall,3), \n",
    "                                                                    (Y_pred==Y_test).sum()/len(Y_pred), round(fscore,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)\n",
    "\n",
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3),\n",
    "        round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 ---- Precision: 1.0 / Recall: 0.263 / Accuracy: 0.89\n",
      "Est: 10 / Depth: 20 ---- Precision: 1.0 / Recall: 0.623 / Accuracy: 0.943\n",
      "Est: 10 / Depth: 30 ---- Precision: 0.985 / Recall: 0.766 / Accuracy: 0.963\n",
      "Est: 10 / Depth: None ---- Precision: 0.985 / Recall: 0.796 / Accuracy: 0.968\n",
      "Est: 50 / Depth: 10 ---- Precision: 1.0 / Recall: 0.305 / Accuracy: 0.896\n",
      "Est: 50 / Depth: 20 ---- Precision: 1.0 / Recall: 0.647 / Accuracy: 0.947\n",
      "Est: 50 / Depth: 30 ---- Precision: 0.984 / Recall: 0.76 / Accuracy: 0.962\n",
      "Est: 50 / Depth: None ---- Precision: 0.986 / Recall: 0.856 / Accuracy: 0.977\n",
      "Est: 100 / Depth: 10 ---- Precision: 1.0 / Recall: 0.216 / Accuracy: 0.882\n",
      "Est: 100 / Depth: 20 ---- Precision: 1.0 / Recall: 0.593 / Accuracy: 0.939\n",
      "Est: 100 / Depth: 30 ---- Precision: 1.0 / Recall: 0.772 / Accuracy: 0.966\n",
      "Est: 100 / Depth: None ---- Precision: 1.0 / Recall: 0.856 / Accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring parameter setting using grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>33.382433</td>\n",
       "      <td>1.005753</td>\n",
       "      <td>0.419795</td>\n",
       "      <td>0.056072</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.974497</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>63.525799</td>\n",
       "      <td>1.492204</td>\n",
       "      <td>0.498793</td>\n",
       "      <td>0.064760</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>58.625582</td>\n",
       "      <td>0.772039</td>\n",
       "      <td>0.434049</td>\n",
       "      <td>0.053354</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>34.001541</td>\n",
       "      <td>1.952088</td>\n",
       "      <td>0.367624</td>\n",
       "      <td>0.029668</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>0.963163</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.973060</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>54.717784</td>\n",
       "      <td>1.637598</td>\n",
       "      <td>0.468078</td>\n",
       "      <td>0.060223</td>\n",
       "      <td>60</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 300}</td>\n",
       "      <td>0.974888</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.972522</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7       33.382433      1.005753         0.419795        0.056072   \n",
       "8       63.525799      1.492204         0.498793        0.064760   \n",
       "11      58.625582      0.772039         0.434049        0.053354   \n",
       "10      34.001541      1.952088         0.367624        0.029668   \n",
       "5       54.717784      1.637598         0.468078        0.060223   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "7               90                150   \n",
       "8               90                300   \n",
       "11            None                300   \n",
       "10            None                150   \n",
       "5               60                300   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.975785   \n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.978475   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.977578   \n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.975785   \n",
       "5     {'max_depth': 60, 'n_estimators': 300}           0.974888   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "7            0.979354           0.975741           0.969452   \n",
       "8            0.976661           0.973944           0.968553   \n",
       "11           0.976661           0.975741           0.967655   \n",
       "10           0.979354           0.974843           0.963163   \n",
       "5            0.974865           0.973944           0.966757   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "7            0.972147         0.974497        0.003399                1  \n",
       "8            0.973046         0.974138        0.003394                2  \n",
       "11           0.973046         0.974138        0.003577                2  \n",
       "10           0.972147         0.973060        0.005458                4  \n",
       "5            0.972147         0.972522        0.003049                5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_features, data['label'])\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting In Python\n",
    "\n",
    "1. Gradient boosting uses boosting whereas Randoom forest uses bagging\n",
    "2. Bagging samples randomly whereas boosting samples based on an increase in weight of what it got wrong previously\n",
    "3. Since all trees of random forest are trained independently they can be parallalized, whereas boosting is iterative\n",
    "4. Random forest uses unweighted scoring, whereas gradient boosting does weighted voting for final prediction\n",
    "5. Easier to tune, harder to overfit in case of random forest whereas gradient boosting is easier to overfit and harder to train, longer to train\n",
    "\n",
    "Then why gradient boosting, more powerful when trained properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "['_SUPPORTED_LOSS', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_check_initialized', '_check_params', '_clear_state', '_estimator_type', '_fit_stage', '_fit_stages', '_get_param_names', '_get_tags', '_init_state', '_is_initialized', '_make_estimator', '_raw_predict', '_raw_predict_init', '_required_parameters', '_resize_state', '_staged_raw_predict', '_validate_estimator', '_validate_y', 'apply', 'decision_function', 'feature_importances_', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params', 'staged_decision_function', 'staged_predict', 'staged_predict_proba']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print(GradientBoostingClassifier())\n",
    "print(dir(GradientBoostingClassifier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GB(est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        est, max_depth, lr, round(precision, 3), round(recall, 3), \n",
    "        round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        for lr in [0.01, 0.1, 1]:\n",
    "            train_GB(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boostin with CV and parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150], \n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "cv_fit = clf.fit(X_tfidf_feat, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
